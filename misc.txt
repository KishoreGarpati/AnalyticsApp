
 
export PATH=$PATH:/usr/hdp/current/kafka-broker/bin

#list kafka tpics



kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic twitterstream
# Go to Kafka directory
 

cd cloudxlab/spark-streaming/kafka
 
/bin/bash put_order_data_in_topic.sh ../data/order_data/ ip-172-31-13-154.ec2.internal:6667 kishore-order-data
 
# Script will push CSV files one by one to Kafka topic after every one minute interval
 
# Let the script run. Do not close the terminal



ip-172-31-29-153.ec2.internal:2181

ip-172-31-13-154.ec2.internal:6667


kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic iot-analytics-data

kafka-topics.sh --zookeeper localhost:2181 --alter --topic iot-sensor-data --config retention.ms=3000

kafka-topics.sh  --list --zookeeper localhost:2181 | grep iot

kafka-console-producer.sh --broker-list ip-172-31-13-154.ec2.internal:6667 --topic iot-analytics-data

kafka-console-consumer.sh --zookeeper localhost:2181 --topic iot-analytics-data --from-beginning

spark-submit --jars spark-streaming-kafka-assembly_2.10-1.6.0.jar spark_streaming_order_status.py localhost:2181 kishore-order-data

join -t , -1 3 -2 3 -o 1.1,1.2,1.3,1.4,2.4,1.5 <(sort -t , -k 3 data/temp_feed.csv) <(sort -t , -k 3 data/cap_feed.csv) | kafka-console-producer.sh --broker-list  ip-172-31-13-154.ec2.internal:6667 --topic iot-sensor-data

spark-submit --jars spark-streaming-kafka-assembly_2.10-1.6.0.jar spark_streaming_order_status.py localhost:2181 raw-data


ip-172-31-13-154.ec2.internal:6667,ip-172-31-29-153.ec2.internal:6667,ip-172-31-38-183.ec2.internal:6667
ip-172-31-13-154.ec2.internal:6667

 ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if']

export PATH=$PATH:/usr/hdp/current/kafka-broker/bin
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kishore-order-one-min-data
 
# Go to spark directory
cd cloudxlab/spark-streaming/spark
 
# Run the Spark Streaming code
 
spark-submit --jars spark-streaming-kafka-assembly_2.10-1.6.0.jar spark_streaming_order_status.py localhost:2181 kishore-order-data


cd /home/kishoregarapati19925005/cloudxlab/spark-streaming/kafka

/bin/bash put_order_data_in_topic.sh ../data/order_data/ ip-172-31-13-154.ec2.internal:6667 kishore-order-data &

cd /home/kishoregarapati19925005/cloudxlab/spark-streaming/spark

spark-submit --jars spark-streaming-kafka-assembly_2.10-1.6.0.jar spark_streaming_order_status.py localhost:2181 kishore-order-data &

cd /home/kishoregarapati19925005/cloudxlab/spark-streaming/node

node index.js &


ps -eaf | grep "put_order_data_in_topic.sh" | grep -v "grep" | awk '{print $2}' | /bin/xargs kill -9





create table BUILDING_LOOKUP
(
BUILDING_ID VARCHAR(10),
BUILDING_ZONE VARCHAR(10),
BUILDING_MGR VARCHAR(50),
BUILDING_AGE INT,
HVAC_PRODUCT VARCHAR(20),
RUNNING_FLAG VARCHAR(10),
LOCATION VARCHAR(50),
TARGET_TEMP INT,
CUR_OCCUP INT,
LAST_UPDATED DATE
);


create table RULES
(
RULE_ID VARCHAR(10),
TEMP_MIN INT,
TEMP_MAX INT,
OCCU_MIN INT,
OCCU_MAX INT,
TRIGGER_VALUE INT
);



LOAD DATA LOCAL INFILE  '/home/kishoregarapati19925005/IOT/building_lookup.csv' INTO TABLE BUILDING_LOOKUP FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n';

trigger_value


hadoop fs -appendToFile - /user/kishoregarapati19925005/sensor_data/data.txt


"update BUILDING_LOOKUP set LAST_UPDATED = NOW(), RUNNING_FLAG = 'Y',TARGET_TEMP=%d,CUR_OCCUP=%d where BUILDING_ID = '%s' and BUILDING_ZONE = '%s'" %(int(trigger_value),int(cap_val),str(build_id),str(zone_id))


kafka-console-producer.sh --broker-list ip-172-31-13-154.ec2.internal:6667 --topic iot-sensor-data


create external table temperature_data(BUILDING_ID string,ZONE_ID string,OLD_TEMP int,NEW_TEMP int,OLD_TEMP int,NEW_OCC int,RUNNING_FLAG string,UPDATED_TIME timestamp)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/kishoregarapati19925005/sensor_data/'


Most occupied building:

select BUILDING_ID,ZONE,_IDsum(NEW_OCC) as OCCUPANCY from temperature_data group by BUILDING_ID,ZONE,_ID order by OCCUPANCY desc;


from pyhive import hive
conn = hive.Connection(host="ip-172-31-13-154.ec2.internal", port=10000, username="hive",password="P9[5Bc_fMh:p")

* * * * * bash /home/kishoregarapati19925005/IOT/hive_schedule.sh >> /home/kishoregarapati19925005/IOT/hive_schedule.log


kafka-console-producer.sh --broker-list ip-172-31-13-154.ec2.internal:6667 --topic iot-analytics-data




#!/bin/bash

cd /home/kishoregarapati19925005/IOT
source venv/bin/activate
python hive_ex.py
deactivate

*/5 * * * * /home/kishoregarapati19925005/IOT/hive_schedule.sh >> /home/kishoregarapati19925005/IOT/hive_schedule.log

for min in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]:
	for b in ['001','002','003','004','005','006']:
	     for z in ['Z01','Z02','Z03','Z04','Z05']:
	        import random
	        import datetime
	        time = str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
	        val = b+','+z+','+b+'_'+z+','+str(random.randint(23,30))+','+time+'\n'
	        filename = 'TEMP_'+'%03d' % min
	        with open (filename, 'a') as f: f.write (val)






#!/bin/bash

export PATH=$PATH:/usr/hdp/current/kafka-broker/bin

for i in {0..29}
do
    TEMP=`printf "TEMP_%03d" $i`
    CAP=`printf "CAP_%03d" $i`
    join -t , -1 3 -2 3 -o 1.1,1.2,1.3,1.4,2.4,1.5 <(sort -t , -k 3 data/$TEMP) <(sort -t , -k 3 data/$CAP) | kafka-console-producer.sh --broker-li
st ip-172-31-13-154.ec2.internal:6667  --topic iot-sensor-data
    sleep 60
done


join -t , -1 3 -2 3 -o 1.1,1.2,1.3,1.4,2.4,1.5 <(sort -t , -k 3 data/TEMP_000) <(sort -t , -k 3 data/CAP_000)





select BUILDING_ID,sum(NEW_OCC) as OCCUPANCY from temperature_data 
group by BUILDING_ID order by BUILDING_ID,ZONE_ID;


select BUILDING_ID,(count(*) * 100 / sum(count(*)) over()) as percentage from temperature_data where running_flag in ('N-Y','Y-Y')
group by BUILDING_ID
order by percentage





select t1.building_id, t1.zone_id, t1.updated_time as run_start
  , (select min(updated_time) 
     from temperature_data t2 
     where t2.building_id = t1.building_id
       and t2.zone_id = t1.zone_id
       and t2.updated_time > t1.updated_time
       and t2.runningstatus = 'N') as run_end
from temperature_data t1
where t1.runningstatus = 'Y'
order by t1.building_id, t1.zone_id, t1.updated_time;




select t1.building_id, t1.zone_id, t1.updated_time as run_start,t2.updated_time as run_end
from temperature_data t1 cross join temperature_data t2 
where t2.building_id = t1.building_id
       and t2.zone_id = t1.zone_id
       and t2.updated_time > t1.updated_time
       and t2.running_flag = 'N'
       and t1.running_flag = 'Y'
       order by t1.building_id, t1.zone_id, t1.updated_time;


temp_feed.csv
cap_feed.csv

join -t , -1 3 -2 3 -o 1.1,1.2,1.3,1.4,2.4,1.5 <(sort -t , -k 3 data/temp_feed.csv) <(sort -t , -k 3 data/cap_feed.csv) | kafka-console-producer.sh --broker-list ip-172-31-13-154.ec2.internal:6667  --topic iot-sensor-data


select sum(CURR_OCCU),BUILIDING_ID, LOCATION from TERMATERURE_LOG, BUILDING_LOOKUP group by BUILDING_ID order by 1 limit 1


select count(RUNNINGFLAG),BUIL,LOC from SUMMARY where RUNNING_FLAG = 'ON' group by buildingd order by 1 limit 1


select count(RUNNINGFLAG),BUIL,LOC from SUMMARY where RUNNING_FLAG = 'ON' group by buildingd order by 1 limit 1



